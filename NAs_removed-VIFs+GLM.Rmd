---
title: ""
author: "Jonathan Gill"
date: "4/22/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Cleaning data and finding collinearity: 
1. testing for and removing (after step 3) aliased variables
2. removing NA cases
3. converting variables to factors
4. converting the response variable to binary, running OLS regression
5. computing VIFs based on the OLS
```{r}
train = read.csv("train.csv")

#population densities (train$density_rural+train$density_suburban+train$density_urban)
densitytest = train$density_rural+train$density_suburban+train$density_urban #Aliased. Should remove 1 or collapse to categorical
sum(densitytest)
length(densitytest)

#marital status
marraigetest = train$married + train$single #All but 7 observations sum to 1. Treat as perfectly correlated
sum(marraigetest)
length(marraigetest)

#home ownership
hometest = train$homeowner+train$renter #around 350 people are neither homeowners nor renters; let's hang on to both variables
sum(hometest)
length(hometest)

#removing missing
train_no_na = train[complete.cases(train), ]

#Converting relevant variables to factors
cols1 = c(2,3,4,5,6,8,11,12,14:45,48)
train_no_na[cols1] = lapply(train_no_na[cols1], factor)


#removing aliased columns
train_no_na = train_no_na[,-which(names(train_no_na) %in% c("density_rural","single"))]

train_no_na$suppdem = ifelse(train_no_na$suppdem=="Y", 1, 0)
#took out density_rural, single, and homeowner
ols = lm(suppdem ~ ., data = train_no_na
)

library(car)
vif(ols)
```


Building GLM with NAs removed
Using the data with NA rows removed, build a GLM with all variables
```{r}
glm = glm(suppdem~ . , family = binomial(), data = train_no_na)
summary(glm)
```

Building GLM with NAs converted to means
1.Using convert to mean function from lecture
2. reload dataset, clean and convert NAs
3. build full GLM
*Note big coeffient changes for ppi, married, num_children, outdgrdn_11, and outdoor_11
```{r}
na.convert.mean = function (frame) 
{
    vars <- names(frame)
    if (!is.null(resp <- attr(attr(frame, "terms"), "response"))) {
        vars <- vars[-resp]
        x <- frame[[resp]]
        pos <- is.na(x)
        if (any(pos)) {
            frame <- frame[!pos, , drop = FALSE]
            warning(paste(sum(pos), "observations omitted due to missing values in the response"))
        }
    }
    for (j in vars) {  #j is variable names
        x <- frame[[j]]
        pos <- is.na(x)
        if (any(pos)) {
            if (length(levels(x))) {   # factors
                xx <- as.character(x)
                xx[pos] <- "NA"
                x <- factor(xx, exclude = NULL)
            }
            else if (is.matrix(x)) {   # matrices
                ats <- attributes(x)
                x.na <- 1*pos
#               x[pos] <- 0
                w <- !pos
                n <- nrow(x)
                TT <- array(1, c(1, n))
                xbar <- (TT %*% x)/(TT %*% w)
                xbar <- t(TT) %*% xbar
                x[pos] <- xbar[pos]
                attributes(x) <- ats
                attributes(x.na) <- ats
                dimnames(x.na)[[2]]=paste(dimnames(x)[[2]],".na",sep='')
                frame[[paste(j,".na",sep='')]] <- x.na 
            } else {   # ordinary numerical vector
                ats <- attributes(x)
                x[pos] <- mean(x[!pos])
#               x[pos] <- 0
                x.na <- 1*pos
                frame[[paste(j,".na",sep='')]] <- x.na 
                attributes(x) <- ats
            }
            frame[[j]] <- x
        }
    }
    frame
}




train = read.csv("train.csv")

train_convert_na = na.convert.mean(train)

#Converting relevant variables to factors
cols1 = c(2,3,4,5,6,8,11,12,14:45,48)
train_convert_na[cols1] = lapply(train_convert_na[cols1], factor)

#removing aliased columns
train_convert_na = train_convert_na[,-which(names(train_convert_na) %in% c("density_rural","single"))]

glm_na_convert = glm(suppdem~ . , family = binomial(), data = train_convert_na)
summary(glm_na_convert)
```


Model building and anova comparison
1. start with only the variables that were significant in the full model
2. add in other coefficients

adding num_children improves on the model

Note: adding age alone improves on the model, but adding age + age.na does not make a significant improvement over adding neither.
```{r}
glm1 = glm(suppdem~ density_suburban + density_urban + sex + combined_ethnicity_4way + combined_ethnicity_4way + ppi + married + education+ hasreligion + catholic + christian + interest_in_religion + donrever_1 + liberal_donor + conservative_donor + contbrel_1 + apparel_1+cat_1+guns_1 + cnty_pct_religious + cnty_pct_evangelical + cnty_pct_religious.na + cnty_pct_evangelical.na, family = binomial(), data = train_convert_na) #significant coefficients only

glm2 = glm(suppdem~ density_suburban + density_urban + sex + combined_ethnicity_4way + combined_ethnicity_4way + ppi + married + education+ hasreligion + catholic + christian + interest_in_religion + donrever_1 + liberal_donor + conservative_donor + contbrel_1 + apparel_1+cat_1+guns_1 + cnty_pct_religious + cnty_pct_evangelical + cnty_pct_religious.na + cnty_pct_evangelical.na
           
           +age + age.na + party_reg_state + party_primary_state + + census_median_income
           
           , family = binomial(), data = train_convert_na) 


glm3 = glm(suppdem~ density_suburban + density_urban + sex + combined_ethnicity_4way + combined_ethnicity_4way + ppi + married + education+ hasreligion + catholic + christian + interest_in_religion + donrever_1 + liberal_donor + conservative_donor + contbrel_1 + apparel_1+cat_1+guns_1 + cnty_pct_religious + cnty_pct_evangelical + cnty_pct_religious.na + cnty_pct_evangelical.na
           
           + num_children
           
           , family = binomial(), data = train_convert_na) 

glm4 = glm(suppdem~ density_suburban + density_urban + sex + combined_ethnicity_4way + combined_ethnicity_4way + ppi + married + education+ hasreligion + catholic + christian + interest_in_religion + donrever_1 + liberal_donor + conservative_donor + contbrel_1 + apparel_1+cat_1+guns_1 + cnty_pct_religious + cnty_pct_evangelical + cnty_pct_religious.na + cnty_pct_evangelical.na
           
           + num_children
           
           , family = binomial(), data = train_convert_na) 

anova(glm1, glm2,  test = "Chisq")
```

Stepwise AIC and BIC
1. mention this was taken from stat 139...
```{r}
require(MASS)
require(stats)

glm_na_convert = glm(suppdem~ . , family = binomial(), data = train_convert_na)
intercept = glm(suppdem~1,family = binomial(), data = train_convert_na)

aicmodel = step(intercept, scope = list(upper = glm_na_convert), direction = "forward", k = 2 )

summary(aicmodel)

```

Prediction
```{r}
test = read.csv("test.csv")
test.converted = na.convert.mean(test)

#Converting relevant variables to factors
cols1 = c(2,3,4,5,6,8,11,12,14:45,48)
test.converted[cols1] = lapply(test.converted[cols1], factor)
out = predict(aicmodel, test.converted)
write.csv(out, "predictions.csv")
```



